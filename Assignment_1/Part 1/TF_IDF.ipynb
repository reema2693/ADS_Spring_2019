{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import collections\n",
    "from tika import parser\n",
    "import csv\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# combined_words_for_tfidf = []\n",
    "#This Function converts the Merged PDF to text\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "# Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "# Process each page contained in the document.\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#By calling this function, it does all the jobs(PDF to text, Filtering Stop_words and creating a CSV file)\n",
    "paths = [r'C:\\Users\\manee\\Desktop\\ADS\\Assignment_1\\Pdf_1.pdf',r'C:\\Users\\manee\\Desktop\\ADS\\Assignment_1\\Pdf_2.pdf',\n",
    "         r'C:\\Users\\manee\\Desktop\\ADS\\Assignment_1\\Pdf_3.pdf',r'C:\\Users\\manee\\Desktop\\ADS\\Assignment_1\\Pdf_4.pdf']\n",
    "combined_lists = []\n",
    "data_lists = []\n",
    "for path in paths:\n",
    "    filtered_sentence = []\n",
    "    content = pdfparser(path)\n",
    "    data_lists.append(content)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', min_df = 0, stop_words = 'english')\n",
    "\n",
    "response = vectorizer.fit_transform(data_lists)\n",
    "\n",
    "weights = np.asarray(response.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_sorted = weights_df.sort_values(by='weight', ascending=False)\n",
    "#if we need only the top 100 keywords within the file, use below\n",
    "#weights_sorted = weights_df.sort_values(by='weight', ascending=False).head(100)\n",
    "\n",
    "weights_sorted.to_csv(path_or_buf=r'C:\\Users\\manee\\Desktop\\ADS\\Assignment_1\\tfidf_WordList.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
