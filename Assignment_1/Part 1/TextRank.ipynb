{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import PyPDF2\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize \n",
    "import collections\n",
    "from tika import parser\n",
    "import csv\n",
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "path = (r'/Users/rajsharavan/Desktop/Python/Splitted PDF/Sample44.pdf')\n",
    "# pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "# pageNumbers = pdfReader.numPages\n",
    "\n",
    "# print(pageNumbers)\n",
    "\n",
    "# textList = []\n",
    "\n",
    "# for i in range(pageNumbers):\n",
    "#     pageObj = pdfReader.getPage(i)\n",
    "#     text = pageObj.extractText()\n",
    "#     #print(text)\n",
    "#     textList.append(text)\n",
    "\n",
    "# print(textList[1])\n",
    "\n",
    "#This Function converts the Merged PDF to text\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = open(data,'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "# Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "# Process each page contained in the document.\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "# pdfparser(path)\n",
    "parsedText = pdfparser(path)\n",
    "#print(parsedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the text\n",
    "import string\n",
    "filtered_sentence = []\n",
    "def clean(parsedText):\n",
    "    #stop_words = set(stopwords.words('english'))   \n",
    "    word_tokens = word_tokenize(parsedText)\n",
    "    words = [w for w in word_tokens if w.isalpha()]\n",
    "    for w in words:\n",
    "        filtered_sentence.append(w.lower())\n",
    "         #if w.lower() not in stop_words:          \n",
    "    return filtered_sentence\n",
    "\n",
    "cleanedText = clean(parsedText)\n",
    "#print(cleanedText)\n",
    "# processedText = word_tokenize(cleanedText)\n",
    "\n",
    "# print (processedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS_Tagging for Lemmatization\n",
    "POS_tag = nltk.pos_tag(cleanedText)\n",
    "#print(POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjectiveTags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatizedText = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjectiveTags:\n",
    "        lemmatizedText.append(str(wordnetLemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatizedText.append(str(wordnetLemmatizer.lemmatize(word[0])))\n",
    "        \n",
    "#print(lemmatizedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging for Filtering\n",
    "\n",
    "POS_tag = nltk.pos_tag(lemmatizedText)\n",
    "\n",
    "#print \"Lemmatized text with POS tags: \\n\"\n",
    "#print (POS_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS Based Filtering\n",
    "\n",
    "stopwords = []\n",
    "\n",
    "wantedPOS = ['NN','NNS','NNP','JJ','JJR','JJS','VBG','FW']\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] not in wantedPOS:\n",
    "        stopwords.append(word[0])\n",
    "\n",
    "punctuations = list(str(string.punctuation))\n",
    "\n",
    "stopwords = stopwords+punctuations\n",
    "\n",
    "stopword_file = open(r'/Users/rajsharavan/Desktop/Python/long_stopwords.txt', 'r')\n",
    "#Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "lots_of_stopwords = []\n",
    "\n",
    "for line in stopword_file.readlines():\n",
    "    lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "stopwords_plus = []\n",
    "stopwords_plus = stopwords + lots_of_stopwords\n",
    "stopwords_plus = set(stopwords_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stopwords\n",
    "processed_text = []\n",
    "for word in lemmatizedText:\n",
    "    if word not in stopwords_plus:\n",
    "        processed_text.append(word)\n",
    "#print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1216\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary Creation which will contain only unique words from processed text\n",
    "vocabulary = list(set(processed_text))\n",
    "#print(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 36....\n"
     ]
    }
   ],
   "source": [
    "#Building a graph [ This is similar to PageRank Algorithm]\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "#All the words in vocabulary list is called as vertices (singular: vertex)\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32) #returns an array of given shape and type filled with zeroes\n",
    "window_size = 3\n",
    "covered_coocurrences = [] #List to hold the weighted edge of each vertices\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i: #No link between the voccabulary and hence its propbability will be initialized to 0\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i]) #Finding the indexes inorder to compare with the existing words within the co-occurences list\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j) #Calculating weights between ecah vertices by formula (1/difference between positions of two vertices) \n",
    "                        covered_coocurrences.append([index_of_i,index_of_j]) #math.fabs gives the absolute value\n",
    "                        \n",
    "#Calculating the weighted summation of connections of a vertex\n",
    "\n",
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]  #sum of each edge connections of words at i and j\n",
    "\n",
    "#Scoring Vertices\n",
    "\n",
    "#The formula used for scoring a vertex represented by i is:\n",
    "    #Score[i] = (1-d) * Summation(j)\n",
    "    #j belongs to list of each vertices that has a connection with i\n",
    "    #summation of j means  Sum of (weighted_edge[i][j]/inout[j])*score[j]\n",
    "    \n",
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold:\n",
    "        #convergence condition---- This means where the score is almost equal to zero i.e a threshold condition\n",
    "        print (\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break\n",
    "\n",
    "import csv\n",
    "import collections\n",
    "\n",
    "def check():\n",
    "    filtered_sentence1 = []\n",
    "    for i in range(0,vocab_len):\n",
    "        test1=(vocabulary[i], str(score[i]))\n",
    "        filtered_sentence1.append(test1)\n",
    "    #print(filtered_sentence1)\n",
    "        #print(type(test1))\n",
    "        #print('Score of',vocabulary[i],':', str(score[i]))\n",
    "    def get_count(word_count_tuple):\n",
    "        return word_count_tuple[1]\n",
    "\n",
    "    word_counts1 = collections.Counter(filtered_sentence1)\n",
    "\n",
    "# This function sorts the words based on the number of times repeated and saves them in a csv file\n",
    "    def print_top():\n",
    "        word_count1 = word_counts1\n",
    "        items = sorted(word_count1.items(),key=get_count,reverse=True)\n",
    "        with open('/Users/rajsharavan/Desktop/Python/Splitted PDF/Textrank/TextRank_Sample44.csv','w') as out:\n",
    "            csv_out=csv.writer(out)\n",
    "            #csv_out.writerow('Name')\n",
    "            for row in items:\n",
    "                csv_out.writerow(row)\n",
    "\n",
    "    print_top()  \n",
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
