{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from xlwt import Workbook\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "contents = []\n",
    "bank_name= []\n",
    "job_url= []\n",
    "list_unigram=[]\n",
    "list_bigram=[]\n",
    "list_trigram=[]\n",
    "bigram_words= []\n",
    "bigram_final= []\n",
    "trigram_final= []\n",
    "cluster_name= []\n",
    "cluster_count= []\n",
    "column_count_value= []\n",
    "cluster_new= []\n",
    "\n",
    "#Opens the CSV file from Amazon S3 and appends the values according to the column\n",
    "client = boto3.client('s3',aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
    "obj_job = client.get_object(Bucket='ads-assignment', Key='Job_Description.csv')\n",
    "df1 = pd.read_csv(obj_job['Body'])\n",
    "for i in df1.columns:\n",
    "    i\n",
    "for ba in df1['Bank Name']:\n",
    "     bank_name.append(ba)\n",
    "for ur in df1['URL']:\n",
    "     job_url.append(ur)\n",
    "for de in df1['Description']:\n",
    "    contents.append(de)\n",
    "\n",
    "#Opens a new workbook\n",
    "wb = Workbook()\n",
    "add = wb.add_sheet('Final_Count', cell_overwrite_ok=True)\n",
    "row=1\n",
    "col=6\n",
    "add.write(0,0, 'Bank Name')\n",
    "add.write(0,1, 'URL')\n",
    "add.write(0,2, 'Description')\n",
    "add.write(0,3, 'Cluster Category')\n",
    "add.write(0,4, 'Job Type')\n",
    "add.write(0,5, 'Focused Area')\n",
    "        \n",
    "#Writes column names in the excel by taking file from Amazon S3\n",
    "client = boto3.client('s3',aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
    "obj = client.get_object(Bucket='ads-assignment', Key='Cluster.csv')\n",
    "df = pd.read_csv(obj['Body'])\n",
    "df=df.fillna(\" \")\n",
    "for i in df.columns:\n",
    "    column_count_value.append(i)\n",
    "    add.write(0,col,i)\n",
    "    col = col +1\n",
    "col=6\n",
    "    \n",
    "#Writes Bank names in the excel sheet at column 0\n",
    "for bankname in bank_name[:]:\n",
    "    bankname\n",
    "    add.write(row,0,bankname)\n",
    "    row = row +1    \n",
    "    \n",
    "row=1\n",
    "#Writes URL in the excel sheet at column 1\n",
    "for job_link in job_url[:]:\n",
    "    job_link\n",
    "    add.write(row,1,job_link)\n",
    "    row=row+1\n",
    "    \n",
    "row=1\n",
    "#Writes Job Desription in the excel sheet at column 2 and does the word count\n",
    "for jobdata in contents[:]:\n",
    "    add.write(row,2,jobdata)\n",
    "    word_tokens1 = word_tokenize(jobdata)\n",
    "    words1 = [w for w in word_tokens1 if w.isalpha()]\n",
    "    words_lowercase = [x.lower() for x in words1]\n",
    "    \n",
    "#Converting the description to Bigrams and Trigrams\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    nltk_tokens = nltk.word_tokenize(jobdata)\n",
    "    words2 = [u for u in nltk_tokens if u.isalpha()]\n",
    "    words2_lowercase = [y.lower() for y in words2]\n",
    "    for z in words2_lowercase: \n",
    "        if z.lower() not in stop_words: \n",
    "            bigram_words.append(z)\n",
    "    bigram = list(nltk.bigrams(bigram_words))\n",
    "    trigram = list(nltk.trigrams(bigram_words))\n",
    "\n",
    "#Formatting both the bigrams and trigrams\n",
    "    for a in bigram:\n",
    "        b=a[0]+\" \"+a[1]\n",
    "        bigram_final.append(b)\n",
    "    for c in trigram:\n",
    "        d=c[0]+\" \"+c[1]+\" \"+c[2]\n",
    "        trigram_final.append(d)\n",
    "        \n",
    "#Opens the cluster csv files from Amazon S3 and clasifies the job to different clusters based on the max number of words repeated in the cluster        \n",
    "    client = boto3.client('s3',aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
    "    obj = client.get_object(Bucket='ads-assignment', Key='Cluster.csv')\n",
    "    df = pd.read_csv(obj['Body'])\n",
    "    df=df.fillna(\" \")\n",
    "    for i in df.columns:\n",
    "        i\n",
    "        for j in df[i]:\n",
    "            if j != \" \":\n",
    "                splitted = len(j.split())\n",
    "                if splitted == 1:\n",
    "                    list_unigram.append(j)\n",
    "                elif splitted ==2:\n",
    "                    list_bigram.append(j)\n",
    "                else:\n",
    "                    list_trigram.append(j)\n",
    "\n",
    "        for k in list_unigram:\n",
    "            count=0\n",
    "            for num1 in words_lowercase:\n",
    "                if k == num1:\n",
    "                    count = count+1\n",
    "            if count!=0:\n",
    "                general_count = (i,count)\n",
    "                general_count1 = (i,k,count)\n",
    "                cluster_name.append(i)\n",
    "                cluster_count.append(general_count)\n",
    "                cluster_new.append(general_count1)\n",
    "                \n",
    "        for l in list_bigram:\n",
    "            count1=0\n",
    "            for num1_bi in bigram_final:\n",
    "                if l == num1_bi:\n",
    "                    count1 = count1+1\n",
    "            if count1!=0:\n",
    "                general_count_bi = (i,count1)\n",
    "                general_count_bi1 = (i,l,count1)\n",
    "                cluster_name.append(i)\n",
    "                cluster_count.append(general_count_bi)\n",
    "                cluster_new.append(general_count_bi1)\n",
    "                \n",
    "        for m in list_trigram:\n",
    "            count2=0\n",
    "            for num1_tri in trigram_final:\n",
    "                if m == num1_tri:\n",
    "                    count2 = count2+1\n",
    "            if count2!=0:\n",
    "                general_count_tri = (i,count2)\n",
    "                general_count_tri1 = (i,m,count2)\n",
    "                cluster_name.append(i)\n",
    "                cluster_count.append(general_count_tri)\n",
    "                cluster_new.append(general_count_tri1)\n",
    "               \n",
    "        del list_unigram[:]\n",
    "        del list_bigram[:]\n",
    "        del list_trigram[:]\n",
    "        \n",
    "#Getting the Cluster names and checking how many words got repeated in that particular cluster    \n",
    "    cluster_dict = {clus:cluster_name.count(clus) for clus in cluster_name}\n",
    "    \n",
    "    data = {k:v for k, v in cluster_dict.items() if v == max(cluster_dict.values())}\n",
    "    multiple_cluster_dict = {}\n",
    "    for (key, value) in cluster_count:\n",
    "        if key in data:\n",
    "            if key in multiple_cluster_dict:\n",
    "                multiple_cluster_dict[key].append(value)\n",
    "            else:\n",
    "                multiple_cluster_dict[key] = [value]\n",
    "    \n",
    "#If only one cluster is having max number of repeated words, it will take that particular cluster for that particular\n",
    "#job and also it prints the focused area(Word which got repeated max times in the final cluster group)\n",
    "    if len(data) ==1:\n",
    "        final_cluster_name=(max(zip(cluster_dict.values(), cluster_dict.keys())))\n",
    "        word_for_cluster={}\n",
    "        for clus in cluster_new:\n",
    "            if final_cluster_name[1] == clus[0]:\n",
    "                word_for_cluster[clus[1]]=clus[2]\n",
    "        word_cluster =(max(zip(word_for_cluster.values(), word_for_cluster.keys())))\n",
    "        add.write(row,3,final_cluster_name[1])\n",
    "        add.write(row,4,'Fintech')\n",
    "        add.write(row,5,word_cluster[1])\n",
    "        \n",
    "#If two or more cluster are having same number of repeated words, it will take the overall count for those clusters \n",
    "#and then prints the cluster which is having max count. Also it prints the focused area(Word which got repeated\n",
    "#max times in the final cluster group)\n",
    "    elif len(data) >1:\n",
    "        sum_value = {k:sum(v) for k,v in multiple_cluster_dict.items()}\n",
    "        final_cluster_name1=(max(zip(sum_value.values(), sum_value.keys())))\n",
    "        word_for_cluster1={}\n",
    "        for clus in cluster_new:\n",
    "            if final_cluster_name1[1] == clus[0]:\n",
    "                word_for_cluster1[clus[1]]=clus[2]\n",
    "        word_cluster1 =(max(zip(word_for_cluster1.values(), word_for_cluster1.keys())))\n",
    "        add.write(row,3,final_cluster_name1[1])\n",
    "        add.write(row,4,'Fintech')\n",
    "        add.write(row,5,word_cluster1[1])\n",
    "        \n",
    "    else:\n",
    "        add.write(row,3,'Others')\n",
    "        add.write(row,4,'Non-Fintech')\n",
    "        add.write(row,5,'Others')\n",
    "\n",
    "\n",
    "#Creating a dictionary with all the cluster words and assigning the value 0\n",
    "    cluster_dict_column = {clus_name:0 for clus_name in column_count_value}\n",
    "\n",
    "    def replace_value(key_to_find, new_value):\n",
    "        '''This function helps us to replace the total number of counts for each Cluster depending on the output(cluster_dict). \n",
    "        It will update the dictionary- cluster_dict_column. Since it is used to write data in final csv file'''\n",
    "        for key in cluster_dict_column.keys():\n",
    "            if key == key_to_find:\n",
    "                cluster_dict_column[key] = new_value\n",
    "\n",
    "    for (key, value) in cluster_dict.items():\n",
    "        replace_value(key, value)\n",
    "        \n",
    "#Printing the count value for each cluster          \n",
    "    for val,num in cluster_dict_column.items():\n",
    "        add.write(row,col,num)\n",
    "        col=col+1\n",
    "        if col == 18:\n",
    "            col=6\n",
    "            row=row+1\n",
    "        \n",
    "    del words_lowercase[:]\n",
    "    del cluster_name[:]\n",
    "    del cluster_new[:]\n",
    "    del cluster_count[:]\n",
    "    del bigram_final[:]\n",
    "    del bigram_words[:]\n",
    "    del bigram[:]\n",
    "    del trigram[:]\n",
    "    del trigram_final[:]\n",
    "    \n",
    "wb.save(os.getcwd()+'/Part2.csv')\n",
    "\n",
    "s3 = boto3.resource('s3',aws_access_key_id=aws_id, aws_secret_access_key=aws_secret)\n",
    "bucket = s3.Bucket('ads-assignment')\n",
    "s3.Object('ads-assignment', 'Part2.csv').put(Body=open(os.getcwd()+'/Part2.csv', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
